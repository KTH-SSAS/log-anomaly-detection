{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHjrroZ3KHgR"
   },
   "source": [
    "*New and tidied up notebook for faster and easier testing of the models*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1825,
     "status": "ok",
     "timestamp": 1616791087071,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "-jHRSWrKAiLV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm # Comment out if tqdm is not installed\n",
    "\n",
    "import train_model\n",
    "from log_analyzer.model.auxiliary import EarlyStopping\n",
    "import log_analyzer.data.data_loader as data_utils\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "# torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7-RVMKFGxV_"
   },
   "source": [
    "# Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_args(bidir, tiered, token_level):\n",
    "    \"\"\"Prepares a Namespace of the same format and fields as created by argparse when parsing user input from the commandline.\n",
    "       (Allows the use of functions from train_model for easier model training)\"\"\"\n",
    "    # Common args (defaults, can be changed)\n",
    "    args = Namespace(\n",
    "        batch_size = 64,\n",
    "        lstm_layers = [128],\n",
    "        context_layers = [128],\n",
    "        embed_dim = 128,\n",
    "        model_dir = 'runs',\n",
    "        load_from_checkpoint = ''\n",
    "    )\n",
    "\n",
    "    args.bidirectional = bidir\n",
    "    args.tiered = tiered\n",
    "\n",
    "    if token_level == 'word':\n",
    "        args.data_folder=os.getcwd() + '/notebooks/data_examples/lanl/lm_feats/word_day_split'\n",
    "        args.jagged = False\n",
    "        args.config = os.getcwd() + '/notebooks/safekit/features/specs/lm/lanl_word_config.json'\n",
    "    elif token_level == 'char':\n",
    "        args.data_folder=os.getcwd() + '/notebooks/data_examples/lanl/lm_feats/raw_day_split'\n",
    "        args.jagged = True\n",
    "        args.config = os.getcwd() + '/notebooks/safekit/features/specs/lm/lanl_char_config.json'\n",
    "    else:\n",
    "        print(\"Error: unexpected token_level, args not prepared.\")\n",
    "        return\n",
    "\n",
    "    ### If model is both fwd and word tokenized\n",
    "    if token_level == 'word' and not args.bidirectional:\n",
    "        args.skipsos = True\n",
    "    else:\n",
    "        args.skipsos = False\n",
    "    \n",
    "    # Return the prepared args\n",
    "    return args\n",
    "    \n",
    "# Note: hyperparameters, such as learning rate and patience, are read in from the config file.\n",
    "#       (Change them there if you want to try other values)\n",
    "\n",
    "# Specify the configuration of the model\n",
    "# All combinations currently supported except bidir + tiered\n",
    "bidir = False\n",
    "tiered = True\n",
    "token_level = 'word' # 'word' or 'char'\n",
    "\n",
    "# Get the args for the model version defined above\n",
    "args = set_args(bidir, tiered, token_level)\n",
    "\n",
    "# Prepare a dataloader for the data\n",
    "with open(args.config, 'r') as f:\n",
    "    conf = json.load(f)\n",
    "sentence_length = conf[\"sentence_length\"] - 1 - int(args.skipsos) + int(args.bidirectional)\n",
    "train_days = conf['train_files']\n",
    "test_days = conf['test_files']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZZoblRnxAmS"
   },
   "source": [
    "\n",
    "# Train model\n",
    "### (One epoch, full dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11235,
     "status": "ok",
     "timestamp": 1616791096516,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "TuqWq1c6xBTb",
    "outputId": "23c787ba-c1df-44be-bfd8-0ccf5f9b2d23",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Create and train the model\n",
    "trainer = train_model.create_model(args)\n",
    "train_losses, test_losses = train_model.train(args, trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 11229,
     "status": "ok",
     "timestamp": 1616791096517,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "wgNW3kYvxD85",
    "outputId": "6c6abf92-1313-4a07-8d68-1d58bde4bd6b"
   },
   "outputs": [],
   "source": [
    "# Plot the train loss over time\n",
    "plt.plot(train_losses)\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('x - Number of batch')\n",
    "# naming the y axis\n",
    "plt.ylabel('y - Average loss')\n",
    "plt.title(\"Training losses\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Final test loss (avg): {np.mean(test_losses)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkOOqtkYxGWy"
   },
   "source": [
    "# Spot check model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11223,
     "status": "ok",
     "timestamp": 1616791096518,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "GmgoSqb5xINj",
    "outputId": "1d838f19-479e-4e05-8fb4-b883894c6c03"
   },
   "outputs": [],
   "source": [
    "args.batch_size = 1\n",
    "_, test_loader = data_utils.load_data(train_days, test_days, args, sentence_length)\n",
    "\n",
    "# Grab the first batch from the test_loader\n",
    "for batch in test_loader:\n",
    "    break\n",
    "\n",
    "loss, output = trainer.eval_step(batch)\n",
    "# The input/output of tiered models have an additional dimension, which we have to squeeze out\n",
    "preds = torch.argmax(output[0], dim=-1)[0] if args.tiered else torch.argmax(output[0], dim=-1)\n",
    "gt = batch['t'][0][0][:len(preds)] if args.tiered else batch['t'][0][:len(preds)]\n",
    "\n",
    "# Get the ground truth ('t') of the first line in the batch extracted above\n",
    "print(f'Ground truth: {gt}')\n",
    "print(f'Model prediction: {preds}')\n",
    "print(f'Loss: {loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gdp0FidFQhu3"
   },
   "source": [
    "\n",
    "# Overfit model\n",
    "\n",
    "### (100+ epochs, 1-10 log lines, train=test set)\n",
    "\n",
    "### (By overfitting LSTM model on a small dataset, let me check whether the model has ability to learn the relation between input and output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 11534,
     "status": "ok",
     "timestamp": 1616791096836,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "-1B5aPzMxJlT",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Set batch_size to 1 so we train on a single line only\n",
    "args.batch_size = 1\n",
    "train_loader, _ = data_utils.load_data(train_days, test_days, args, sentence_length)\n",
    "\n",
    "trainer = train_model.create_model(args)\n",
    "\n",
    "for batch in train_loader:\n",
    "    break\n",
    "\n",
    "# Disable verbose of the early_stopping object to avoid large amounts of output\n",
    "trainer.early_stopping.verbose = False\n",
    "train_losses = []\n",
    "epochs = 500\n",
    "# tqdm provides a nice and minimal progress bar + time estimate for the loop\n",
    "# If tqdm is not installed, simply comment the line out and use the alternatives below instead\n",
    "#for i in range(epochs):\n",
    "#    if i % 250 == 0:\n",
    "#        print('Epoch: {i}')\n",
    "for i in tqdm(range(epochs)):\n",
    "    loss, _ = trainer.train_step(batch) # We ignore the early_stopping flag\n",
    "    train_losses.append(loss.item())\n",
    "\n",
    "trainer.early_stopping.verbose = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 230183,
     "status": "ok",
     "timestamp": 1616791315494,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "lyMnmxlJKxBP",
    "outputId": "2597d9cc-4d54-4003-99c1-5e451a619137"
   },
   "outputs": [],
   "source": [
    "# Plot the train loss over time\n",
    "plt.plot(train_losses)\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('x - Number of epoch')\n",
    "# naming the y axis\n",
    "plt.ylabel('y - Average loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230515,
     "status": "ok",
     "timestamp": 1616791315830,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "cgNPbcpU57Zp",
    "outputId": "696824d1-e674-421f-c2cd-aa43c965d854"
   },
   "outputs": [],
   "source": [
    "loss, output = trainer.eval_step(batch)\n",
    "# The input/output of tiered models have an additional dimension, which we have to squeeze out\n",
    "preds = torch.argmax(output[0], dim=-1)[0] if args.tiered else torch.argmax(output[0], dim=-1)\n",
    "gt = batch['t'][0][0][:len(preds)] if args.tiered else batch['t'][0][:len(preds)]\n",
    "\n",
    "# Get the ground truth ('t') of the first line in the batch extracted above\n",
    "print(f'Ground truth: {gt}')\n",
    "print(f'Model prediction: {preds}')\n",
    "print(f'Loss: {loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN1rIDpHKr5dyYDqgfJHqEe",
   "collapsed_sections": [],
   "mount_file_id": "1XofCw7SikjyLrq8NHxPt_E_C2eQksVny",
   "name": "Test_code(Dataloader + LSTM + Training loop).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python388jvsc74a57bd08d4f9f00e7711475c21a5f18d320a8828210fd25fad58adf8728543da7a1c125",
   "display_name": "Python 3.8.8 64-bit ('log-data-project': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "metadata": {
   "interpreter": {
    "hash": "8d4f9f00e7711475c21a5f18d320a8828210fd25fad58adf8728543da7a1c125"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}