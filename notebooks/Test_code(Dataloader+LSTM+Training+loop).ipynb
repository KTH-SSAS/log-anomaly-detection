{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vHjrroZ3KHgR"
   },
   "source": [
    "*This is code to test the functionality of LSTM. If this code is messy, sorry in advance... this is my third or second time dealing with Pytorch...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 1825,
     "status": "ok",
     "timestamp": 1616791087071,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "-jHRSWrKAiLV"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from scipy.stats import truncnorm\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import OrderedDict\n",
    "import os\n",
    "import json\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "cuda = torch.cuda.is_available()\n",
    "# torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7-RVMKFGxV_"
   },
   "source": [
    "# Parameter setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 1826,
     "status": "ok",
     "timestamp": 1616791087076,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "U-taqYgpwa-l"
   },
   "outputs": [],
   "source": [
    "#manual setting for parameters\n",
    "cwd = os.getcwd()\n",
    "specpath = cwd + '/safekit/features/specs/lm/'\n",
    "datapath = cwd + '/data_examples/lanl/lm_feats/'\n",
    "layer_list = [10] #hidden units of each layer.\n",
    "lr = 1e-3 #learning rate .\n",
    "embedding_dim = 20 # one word/char will be mapped to this dimension.\n",
    "mb_size = 128 #size of mini batch.\n",
    "maxbadcount = 10\n",
    "patience = 20\n",
    "test = True\n",
    "delimiter = ' '\n",
    "direction = 'fwd' \n",
    "token_level = 'word'\n",
    "tiered = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 1823,
     "status": "ok",
     "timestamp": 1616791087077,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "IvOiQ6jUwfEy"
   },
   "outputs": [],
   "source": [
    "if token_level == 'word':\n",
    "    datafolder = datapath + 'word_day_split/'\n",
    "    config = 'lanl_word_config.json'\n",
    "    jagged = False\n",
    "else:\n",
    "    datafolder = datapath + 'raw_day_split/'\n",
    "    config = 'lanl_char_config.json'\n",
    "    jagged = True\n",
    "\n",
    "if direction == 'fwd':\n",
    "    bid = False\n",
    "else: \n",
    "    bid = True\n",
    "\n",
    "if direction == 'fwd' and token_level == 'word':\n",
    "    skipsos = True\n",
    "else:\n",
    "    skipsos = False\n",
    "\n",
    "conf = json.load(open(specpath + config, 'r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 1819,
     "status": "ok",
     "timestamp": 1616791087077,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "pU9M0LU_wgKr"
   },
   "outputs": [],
   "source": [
    "vocab_size = conf['token_set_size']  \n",
    "weekend_days = conf[\"weekend_days\"]\n",
    "sentence_length = conf['sentence_length'] - 1 - int(skipsos) + int(bid)\n",
    "if test:\n",
    "    files = conf[\"test_files\"] # 5000 lines from each of day 0, day 1 and day 2\n",
    "else:\n",
    "    files = [str(i) + '.txt' for i in range(conf[\"num_days\"]) if i not in weekend_days]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TKtKVxdBwlX7"
   },
   "source": [
    "# Class: Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 1817,
     "status": "ok",
     "timestamp": 1616791087078,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "ZcThzOufwu9k"
   },
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            if self.counter % 10 == 0:\n",
    "                self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}. Best loss: {-self.best_score}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3WovjO0wyFa"
   },
   "source": [
    "# Class: Dataset handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 1814,
     "status": "ok",
     "timestamp": 1616791087079,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "lFqZ7Ld-MGQG"
   },
   "outputs": [],
   "source": [
    "def get_mask(lens, num_tokens):\n",
    "    \"\"\"\n",
    "    For masking output of lm_rnn for jagged sequences for correct gradient update.\n",
    "    Sequence length of 0 will output nan for that row of mask so don't do this.\n",
    "\n",
    "    :param lens: Numpy vector of sequence lengths\n",
    "    :param num_tokens: (int) Number of predicted tokens in sentence.\n",
    "    :return: A numpy array mask MB X num_tokens\n",
    "             For each row there are: lens[i] values of 1/lens[i]\n",
    "                                     followed by num_tokens - lens[i] zeros\n",
    "    \"\"\"\n",
    "    mask_template = torch.arange(num_tokens, dtype=torch.float)\n",
    "    return (mask_template < lens) / lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 1810,
     "status": "ok",
     "timestamp": 1616791087079,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "WjEzp-zJwzO6"
   },
   "outputs": [],
   "source": [
    "class LazyTextDataset(Dataset):\n",
    "    def __init__(self, filename, sentence_length, skipsos, jagged, bidir, delimiter, transform=None):\n",
    "        self._filename = filename\n",
    "        self._total_data = 0\n",
    "        self.transform = transform\n",
    "        self.f = open(filename, 'r')\n",
    "        self._total_data = len(self.f.readlines()) - 1\n",
    "        self.f = open(filename, 'r')\n",
    "        \n",
    "        self.delimiter = delimiter\n",
    "        self.skipsos = skipsos\n",
    "        self.jagged = jagged\n",
    "        self.bidir = bidir\n",
    "        self.sentence_length = sentence_length\n",
    "       \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        l = self.f.readline()\n",
    "        sentence_length = self.sentence_length - 1 - int(self.skipsos) + int(self.bidir)\n",
    "        if l != '':\n",
    "            line = torch.tensor([int(k) for k in l.strip().split(self.delimiter)])\n",
    "            self.endx = len(line) - int(not self.bidir)\n",
    "            self.endt = len(line) - int(self.bidir)\n",
    "\n",
    "            self.datadict = {'line': line[0],\n",
    "                            'second': line[1],\n",
    "                            'day': line[2],\n",
    "                            'user': line[3],\n",
    "                            'red': line[4],\n",
    "                            'x': line[(5 + int(self.jagged) + int(self.skipsos)):self.endx],\n",
    "                            't': line[(6 + int(self.jagged) + int(self.skipsos)):self.endt]}\n",
    "            if self.jagged:\n",
    "                self.datadict['length'] = line[5]\n",
    "                self.datadict['mask'] = get_mask(self.datadict['length'] - 2*int(self.bidir) - int(self.skipsos), self.sentence_length - 2*int(self.bidir))\n",
    "                # assert np.all(self.datadict['lengths'] <= x.get_shape().as_list()[1]), 'Sequence found greater than num_tokens_predicted'\n",
    "                # assert np.nonzero(self.datadict['lengths'])[0].shape[0] == self.datadict['lengths'].shape[0], \\\n",
    "                #     'Sequence lengths must be greater than zero.' \\\n",
    "                #     'Found zero length sequence in datadict[\"lengths\"]: %s' % self.datadict['lengths']  \n",
    "\n",
    "        return self.datadict\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self._total_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data handler for a tiered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineLMBatcher: \n",
    "    # It seems Pytorch dataset and dataloader can't sort users... so I needed to build it from scratch..\n",
    "    def __init__(self, file_path, conf, context_size, skipsos, jagged, bidir, batch_size=100, num_steps=5, delimiter=\" \", skiprows=0):\n",
    "        cols = ['line', 'second', 'day', 'user', 'red'] + [f'x_{i}' for i in range(conf['sentence_length'])]\n",
    "        self.day_df = dd.read_csv(file_path, names=cols, sep = ' ', blocksize=25e3)\n",
    "        self.user_id = [] # set()\n",
    "        self.lst_avail_id = []\n",
    "        self.pre_lst_avail_id = []\n",
    "        self.df_id = {}\n",
    "        self.len_id = {}\n",
    "        self.saved_lstm = {}\n",
    "        self.context_size = context_size\n",
    "        self.sel_part = 0\n",
    "        self.current_num_batch = 0\n",
    "        self.num_steps = num_steps\n",
    "        self.batch_size = batch_size\n",
    "        self.staggler_num_steps = 1\n",
    "        self.jagged = jagged\n",
    "        self.skipsos = skipsos\n",
    "        self.bidir = bidir\n",
    "        self.sentence_length = (conf['sentence_length'] - 1) - int(self.skipsos) + int(self.bidir)\n",
    "        self.empty = False\n",
    "        \n",
    "    def filter_partition(self):\n",
    "        partition = self.day_df.get_partition(self.sel_part)\n",
    "        current_ids = partition.user.drop_duplicates().compute().tolist()\n",
    "        for c_id in current_ids:\n",
    "            if c_id not in self.user_id:\n",
    "                self.df_id[c_id] = None\n",
    "                self.saved_lstm[c_id] = (torch.zeros((self.context_size[0])),\\\n",
    "                                         torch.zeros((len(self.context_size), self.context_size[0])),\\\n",
    "                                         torch.zeros((len(self.context_size), self.context_size[0])))\n",
    "            self.df_id[c_id] = pd.concat([self.df_id[c_id], partition[partition.user == c_id].compute()], axis=0)\n",
    "            self.len_id[c_id] = len(self.df_id[c_id])\n",
    "\n",
    "        self.user_id = current_ids + [usr for usr in self.user_id if usr not in current_ids]\n",
    "        self.sel_part += 1   \n",
    "\n",
    "    def update_len(self):\n",
    "        self.lst_avail_id = []\n",
    "        self.current_num_batch = 0\n",
    "        for j in self.user_id:\n",
    "            above_num_steps = self.len_id[j] >= self.num_steps\n",
    "            self.current_num_batch += above_num_steps\n",
    "            if above_num_steps and j not in self.lst_avail_id:\n",
    "                self.lst_avail_id.append(j)\n",
    "    \n",
    "    def __iter__(self):\n",
    "        while not self.empty:\n",
    "            output = []\n",
    "            datadict = {}\n",
    "            ctxt_vector = torch.tensor([])\n",
    "            h_state = torch.tensor([])\n",
    "            c_state = torch.tensor([])\n",
    "            while output == []:\n",
    "                if self.current_num_batch < self.batch_size and self.sel_part < self.day_df.npartitions: # Read a new partition\n",
    "                    self.filter_partition()\n",
    "                    self.update_len()\n",
    "                elif self.current_num_batch == 0 and self.sel_part == self.day_df.npartitions: # Activate staggler mode\n",
    "                    self.batch_size = self.batch_size * self.num_steps\n",
    "                    self.num_steps = self.staggler_num_steps\n",
    "                    self.sel_part += 1\n",
    "                    self.update_len()\n",
    "                elif self.current_num_batch > 0: # Output data\n",
    "                    for j in self.lst_avail_id[:self.batch_size]:\n",
    "                        output.append(self.df_id[j].iloc[0:self.num_steps].values)                    \n",
    "                        ctxt_vector = torch.cat((ctxt_vector, torch.unsqueeze(self.saved_lstm[j][0], dim = 0)), dim = 0)                    \n",
    "                        h_state = torch.cat((h_state, torch.unsqueeze(self.saved_lstm[j][1], dim = 0)), dim = 0)\n",
    "                        c_state = torch.cat((c_state, torch.unsqueeze(self.saved_lstm[j][2], dim = 0)), dim = 0)\n",
    "                        self.df_id[j] = self.df_id[j].iloc[self.num_steps:, :]\n",
    "                        self.len_id[j] = len(self.df_id[j])\n",
    "                    self.pre_lst_avail_id = self.lst_avail_id\n",
    "                    self.update_len()\n",
    "                    output = torch.tensor(output).long()\n",
    "                    batch = torch.transpose(output, 0, 1)\n",
    "                    endx = batch.shape[2] - int(not self.bidir)\n",
    "                    endt = batch.shape[2] - int(self.bidir)\n",
    "                    datadict = {'line': batch[:, :, 0],\n",
    "                                'second': batch[:, :, 1],\n",
    "                                'day': batch[:, :, 2],\n",
    "                                'user': batch[:, :, 3],\n",
    "                                'red': batch[:, :, 4],\n",
    "                                'x': [batch[0, :, 5 + self.jagged + self.skipsos:endx]] * self.num_steps,\n",
    "                                't': [batch[0, :, 6 + self.jagged + self.skipsos:endt]] * self.num_steps,\n",
    "                                'context_vector': ctxt_vector, #,['context_vector'],\n",
    "                                'c_state_init': torch.transpose(h_state, 0,1), #state_triple['c_state_init'],\n",
    "                                'h_state_init': torch.transpose(c_state, 0,1)} #state_triple['h_state_init']}\n",
    "                    if self.jagged:\n",
    "                        datadict['lens'] = [batch[0, :, 5] - self.skipsos] * self.num_steps\n",
    "                        datadict['masks'] = [get_mask(seq_length - 2 * self.bidir, sentence_length - 2 * self.bidir) for\n",
    "                                             seq_length in datadict['lens']]\n",
    "                else: # Empty dataset.\n",
    "                    self.empty = True\n",
    "                    break\n",
    "\n",
    "            yield datadict\n",
    "    \n",
    "    def update_state(self, ctxt_vectors, h_states, c_states):\n",
    "        ctxt_vectors = ctxt_vectors.data\n",
    "        h_states = torch.transpose(h_states.data, 0,1)\n",
    "        c_states = torch.transpose(c_states.data, 0,1)\n",
    "        for usr, ctxt_v, h_state, c_state in zip(self.pre_lst_avail_id[:self.batch_size], ctxt_vectors, h_states, c_states):\n",
    "            self.saved_lstm[usr] = (ctxt_v, h_state, c_state) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_handler = OnlineLMBatcher(datafolder + files[0], conf, context_size, skipsos, jagged, bid, batch_size=64, num_steps=num_steps, delimiter=\" \", skiprows=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "218"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 217,  227,  230,  113,  146,  147,  149,  156,  161, 1538,  166,  167,\n",
      "          183,  186, 2496,  187,  193,  202,  205,  242, 2510, 1548,  181,  184,\n",
      "         2640,  111, 2096, 1542,  189, 2660,  221, 1621, 2731,  724, 2740, 2741,\n",
      "         2139,  803, 2150, 1500,  747,  764, 2858,  121,  150,  784,  177,  117,\n",
      "         2973,  126,  729,  832,  833,  841,  752,  797, 2088,  725, 1550, 3198,\n",
      "         3288, 2122, 3396, 1507],\n",
      "        [ 218,  228,  231,  114,  732,  148,  742,  157,  759, 1539,  762,  168,\n",
      "          794,  800, 2497,  188,  194, 2505,  206,  243, 2511, 1549,  182,  185,\n",
      "         2728,  112, 2097, 2110,  190, 2661,  222, 1622, 2732, 2093, 2806, 2808,\n",
      "         2745, 2295, 2151, 2790,  748,  765, 2859,  122,  151,  785,  178,  118,\n",
      "         2974,  127, 2979, 3003,  834, 2160,  753,  798, 3136, 2383, 2283, 3199,\n",
      "         3289, 2123, 3397, 2388],\n",
      "        [ 824,  229,  232,  717,  733,  741, 2101,  158, 2483, 1540,  763,  169,\n",
      "          795,  801, 2498,  804,  195, 3153,  207,  839, 2512, 2115, 1573,  799,\n",
      "         2788,  714, 2261, 2111,  191, 2662,  223, 2669, 2733, 2094, 2874, 2809,\n",
      "         3066, 2296, 2756, 2971,  749,  766, 3055,  123,  743,  786,  179, 2972,\n",
      "         2975,  728, 2980, 3004,  835, 3006, 3058, 3065, 3137, 2384, 2284, 3200,\n",
      "         3290, 2409, 3456, 2389]])\n",
      "tensor([[ 208,  825,  837, 2670, 3749,  128,  744,  153,  760, 2803,  170, 2297,\n",
      "          805,  196, 3216,  203, 3473,  233, 2789, 2385, 1506,  159, 1574,  836,\n",
      "          718, 2857, 3197,  719, 3943,  750, 1589,  802, 3995,  740,  180,  796,\n",
      "         4068, 4075, 2390, 2262,  152, 2405, 1569,  715, 2092, 2734, 4161, 2095,\n",
      "         3346, 4176, 2877, 2882, 2387,  716, 3056,  726,  727, 2116,  820,  821,\n",
      "         2580, 2581, 1508,  154],\n",
      "        [ 209,  826,  838, 3081, 3750,  129,  745,  746,  761, 2804,  768, 2747,\n",
      "          806,  197, 3468,  204, 3525,  234, 3054, 2386, 3861,  160, 1575, 3005,\n",
      "         1501, 3511, 3335,  720, 3996,  751, 3148, 1592, 4064, 1516, 2880, 1577,\n",
      "         4069, 4165, 3455, 2263, 1532, 2406, 1570, 2086, 2976, 2735, 4162, 2978,\n",
      "         3521, 4270, 4177, 4179, 2977, 2856, 4341, 3269, 3270, 2117, 3282, 3283,\n",
      "         2791, 2582, 3139,  155],\n",
      "        [ 210,  827, 2157, 3082, 3751,  130, 1528, 3757, 1543, 2805,  769, 2748,\n",
      "          807,  198, 3469, 3770, 3771,  235, 3854, 3138, 4074,  757, 1576, 3526,\n",
      "         1502, 3937, 3512,  721, 4667, 1535, 3946, 1593, 4065, 1517, 4007, 1578,\n",
      "         4155, 4251, 4076, 2643, 1533, 2645, 1571, 2087, 4158, 3263, 4246, 3201,\n",
      "         4174, 4358, 4271, 4600, 4250, 4340, 4665, 4345, 4346, 2118, 4368, 4369,\n",
      "         4443, 4444, 4447,  754]])\n",
      "tensor([[ 211,  828, 2158, 3291, 3752,  131, 1529, 1544, 3206,  770, 2749, 2499,\n",
      "          199, 3470, 4604,  236, 3855, 3264,  758, 2412, 1503, 3513,  722, 1536,\n",
      "         1594, 4066, 1518, 1579, 4156, 4254, 2644, 1534, 2646, 1572, 2641, 3451,\n",
      "         4247, 3517, 2119,  755, 1568, 2513,  734, 3463, 2757, 1523, 1541, 4610,\n",
      "         2472, 2504, 2506, 3189,  192,  119,  124, 2265, 2484, 2408, 2417,  224,\n",
      "          115, 3620,  140,  144],\n",
      "        [ 212,  829, 2309, 3292, 3857,  132, 1530, 1545, 3207,  771, 2750, 2500,\n",
      "          200, 3633, 4605,  237, 3856, 3265, 2482, 2413, 1504, 3625,  723, 1537,\n",
      "         1595, 4067, 1519, 1580, 4157, 4511, 2736, 2271, 2647, 2127, 2642, 3452,\n",
      "         4248, 3628, 2120,  756, 3145, 2514,  735, 3697, 2997, 1524, 2401, 4611,\n",
      "         3260, 3284, 3285, 3190, 2420,  120,  125, 2266, 2583, 2489, 2591,  831,\n",
      "          116, 3621,  141,  145],\n",
      "        [ 213, 1611, 2426, 3359, 3858,  133, 1531, 1546, 3344,  772, 3149, 2501,\n",
      "          201, 3634, 4912,  238, 4758, 3266, 3864, 2414, 2259, 3626, 1505, 2107,\n",
      "         2140, 4151, 1520, 1581, 4243, 4512, 2737, 2272, 2868, 2878, 3132, 4159,\n",
      "         4249, 3629, 2121, 2109, 4461, 2515,  736, 4525, 2998, 1525, 2402, 4681,\n",
      "         4759, 4777, 4778, 3191, 2657, 3576, 2860, 2267, 2867, 2584, 2746, 2663,\n",
      "         3575, 3622,  142,  731]])\n",
      "tensor([[2102, 2648,  773,  175, 2415, 2502, 5085,  808,  214, 1612, 2427, 3360,\n",
      "         2260, 4152, 5139, 3267, 2863, 1521, 1526, 1582, 2475, 5243, 3345, 2281,\n",
      "         3144, 3863, 2403, 2869, 4678, 3068, 2893, 3623, 5429, 4163, 4363, 3150,\n",
      "         4666,  134, 4818, 5541, 5545, 3271, 3272, 3273, 5607, 5609, 4682, 5690,\n",
      "         4981, 2738,  165, 5821, 4160, 3057, 2108,  840,  767, 4918, 6041, 6051,\n",
      "         3192, 2601,  830, 3527],\n",
      "        [2103, 2802,  774,  176, 2588, 2655, 5086,  809,  215, 1613, 2428, 3597,\n",
      "         2473, 4153, 5341, 3268, 3579, 1522, 1527, 1583, 2476, 5244, 3460, 2282,\n",
      "         5253, 4827, 2404, 3060, 5357, 3069, 2894, 5427, 5430, 4164, 4364, 3464,\n",
      "         5517,  135, 5525, 5542, 6174, 3940, 3941, 3942, 5608, 5610, 4683, 5691,\n",
      "         4982, 2739, 2407, 5822, 4960, 5825, 2273, 2509, 5977, 4919, 6134, 6154,\n",
      "         3193, 2602, 2598, 3884],\n",
      "        [2104, 2870,  775,  793, 2589, 2656, 5087,  810,  216, 1614, 2429, 3710,\n",
      "         2474, 4154, 5342, 3453, 5142, 3997, 2100, 1584, 5241, 5434, 3461, 2585,\n",
      "         5254, 5349, 2485, 3061, 5537, 3471, 2895, 5428, 5431, 5432, 5443, 3465,\n",
      "         5518,  136, 5526, 5543, 6255, 5603, 5604, 5605, 5907, 5611, 4684, 5692,\n",
      "         4983, 2792, 5787, 5901, 4961, 5826, 2274, 5845, 5978, 5988, 6135, 6155,\n",
      "         3194, 2603, 6251, 6252]])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(data_handler):\n",
    "    print(data['line'])\n",
    "    if i == 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B5GDnhW1w1Vj"
   },
   "source": [
    "# Class: LSTM Models (To-do: tiered model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 2115,
     "status": "ok",
     "timestamp": 1616791087388,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "CfBrtJ5Cw2qS"
   },
   "outputs": [],
   "source": [
    "\n",
    "def truncated_normal_(tensor, mean=0, std=1):\n",
    "    size = tensor.shape\n",
    "    tmp = tensor.new_empty(size + (4,)).normal_()\n",
    "    valid = (tmp < 2) & (tmp > -2)\n",
    "    ind = valid.max(-1, keepdim=True)[1]\n",
    "    tensor.data.copy_(tmp.gather(-1, ind).squeeze(-1))\n",
    "    tensor.data.mul_(std).add_(mean)\n",
    "    return tensor    \n",
    "\n",
    "def initialize_weights(net, initrange = 1.0):\n",
    "    for m in net.modules():\n",
    "        if isinstance(m, nn.Linear):\n",
    "            initrange *= 1.0/np.sqrt(m.weight.data.shape[1])\n",
    "            m.weight.data = initrange * truncated_normal_(m.weight.data, mean = 0.0, std =1)\n",
    "            m.bias.data.zero_()\n",
    "        elif isinstance(m, nn.Embedding):\n",
    "            truncated_normal_(m.weight.data, mean = 0.0, std =1)\n",
    "\n",
    "\n",
    "class Fwd_LSTM(nn.Module):\n",
    "    def __init__(self, layers, vocab_size, embedding_dim, jagged = False, tiered =False, context_vector_size = 0):\n",
    "        super().__init__()\n",
    "        self.layers = layers \n",
    "        self.jagged = jagged\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tiered = tiered\n",
    "        self.bid = False\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        if self.tiered:\n",
    "            self.embedding_dim += context_vector_size  \n",
    "\n",
    "#         self.stacked_lstm = build_stacked_lstm(self.layers, self.embedding_dim, self.bid)\n",
    "        self.stacked_lstm = nn.LSTM(self.embedding_dim, self.layers[0], len(self.layers), batch_first = True, bidirectional = self.bid)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.hidden2tag = nn.Linear(self.layers[-1], self.vocab_size)\n",
    "\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, sequences, lengths = None, context_vectors = None):\n",
    "        x_lookups = self.embeddings(sequences)  # batch x seq len x embedding\n",
    "        if self.tiered:\n",
    "            cat_x_lookups = torch.tensor([])\n",
    "            x_lookups= x_lookups.transpose(0,1) #  seq len x batch x embedding\n",
    "            for x_lookup in x_lookups: #for each batch x embedding.\\\n",
    "                x_lookup = torch.unsqueeze(torch.cat((x_lookup, context_vectors), dim=1), dim=0) # 1 x batch x embedding\n",
    "                cat_x_lookups = torch.cat((cat_x_lookups, x_lookup), dim = 0) # concatenate so n x batch x embedding (n length of a concatenated sequence)\n",
    "            x_lookups = cat_x_lookups.transpose(0,1) #batch x seq len x embedding + context \n",
    "            \n",
    "        lstm_in = x_lookups\n",
    "        if self.jagged:\n",
    "            lstm_in = pack_padded_sequence(lstm_in, lengths, batch_first=True)\n",
    "        lstm_out, (hx, cx)  = self.stacked_lstm(x_lookups)\n",
    "        if self.jagged: \n",
    "            lstm_out = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "\n",
    "        output = self.tanh(lstm_out)\n",
    "        tag_size = self.hidden2tag(output)\n",
    "        return tag_size, lstm_out, hx\n",
    "    \n",
    "class Bid_LSTM(nn.Module):\n",
    "    def __init__(self, layers, vocab_size, embedding_dim, jagged = False, tiered =False, context_vector_size = 0):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.layers = layers \n",
    "        self.jagged = jagged\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.tiered = tiered\n",
    "        self.bid = True\n",
    "        \n",
    "        self.embeddings = nn.Embedding(self.vocab_size, self.embedding_dim)\n",
    "        \n",
    "        if self.tiered:\n",
    "            self.embedding_dim += context_vector_size\n",
    "\n",
    "        self.stacked_bid_lstm = nn.LSTM(self.embedding_dim, self.layers[0], len(self.layers), batch_first = True, bidirectional = self.bid)\n",
    "        self.tanh = nn.Tanh()\n",
    "        self.hidden2tag = nn.Linear(self.layers[-1] * 2, self.vocab_size)\n",
    "\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, sequences, lengths = None, context_vectors = None):   \n",
    "        x_lookups = self.embeddings(sequences) #batch size, sequence length, embedded dimension\n",
    "        if self.tiered:\n",
    "            x_lookups = torch.cat(x_lookups, context_vectors, dim=2)\n",
    "\n",
    "        lstm_in = x_lookups\n",
    "        if self.jagged:\n",
    "            lstm_in = pack_padded_sequence(lstm_in, lengths, batch_first=True)            \n",
    "            \n",
    "        lstm_out, (hx, cx)  = self.stacked_bid_lstm(x_lookups)\n",
    "        if self.jagged:\n",
    "            lstm_out = pad_packed_sequence(lstm_out, batch_first=True)\n",
    "        \n",
    "        output = self.tanh(lstm_out)\n",
    "        tag_size = self.hidden2tag(output)\n",
    "           \n",
    "        return tag_size, lstm_out, hx\n",
    "    \n",
    "\n",
    "class Context_LSTM(nn.Module):\n",
    "    def __init__(self, ctxt_lv_layers, input_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.ctxt_lv_layers = ctxt_lv_layers \n",
    "        self.input_dim = input_dim\n",
    "        self.context_lstm_layers = nn.LSTM(self.input_dim, self.ctxt_lv_layers[0], len(ctxt_lv_layers), batch_first = True, bidirectional = bid)\n",
    "        \n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, lower_lv_outputs, final_hidden, context_h, context_c, seq_len = None):   \n",
    "        if seq_len is not None:\n",
    "            mean_hidden = torch.sum(lower_lv_outputs, dim = 1) / seq_len\n",
    "        else:\n",
    "            mean_hidden = torch.mean(lower_lv_outputs, dim = 1)\n",
    "        cat_input = torch.cat((mean_hidden, final_hidden[-1]), dim=1)\n",
    "        synthetic_input = torch.unsqueeze(cat_input, dim = 1)\n",
    "\n",
    "        output, (context_hx, context_cx) = self.context_lstm_layers(synthetic_input, (context_h, context_c))\n",
    "        return output, context_hx, context_cx \n",
    "    \n",
    "class Tiered_LSTM(nn.Module):\n",
    "    def __init__(self, low_lv_layers, ctxt_lv_layers, vocab_size, embedding_dim, context_vector_size, \n",
    "                 jagged = False, bid = False):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.bid = bid\n",
    "        if self.bid:\n",
    "            self.model = Bid_LSTM\n",
    "        else:\n",
    "            self.model = Fwd_LSTM\n",
    "        self.low_lv_layers = low_lv_layers \n",
    "        self.ctxt_lv_layers = ctxt_lv_layers\n",
    "        self.jagged = jagged\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        self.low_lv_lstm = self.model(self.low_lv_layers, self.vocab_size, self.embedding_dim, \n",
    "                                      jagged = self.jagged, tiered = True, context_vector_size = self.ctxt_lv_layers[-1])\n",
    "        self.ctxt_lv_lstm = Context_LSTM(self.ctxt_lv_layers, low_lv_layers[-1] * 2)\n",
    "\n",
    "        initialize_weights(self)\n",
    "\n",
    "    def forward(self, user_sequences, context_vectors, context_h, context_c, lengths = None):\n",
    "        self.ctxt_vector = context_vectors\n",
    "        self.ctxt_h = context_h\n",
    "        self.ctxt_c = context_c\n",
    "        tag_output = []\n",
    "        for sequences in user_sequences: #number of steps (e.g., 3), number of users (e.g., 64), lengths of sequences (e.g., 10)\n",
    "            tag_size, low_lv_lstm_outputs, final_hidden = self.low_lv_lstm(sequences, lengths = lengths, context_vectors = self.ctxt_vector)\n",
    "            self.ctxt_vector, self.ctxt_h, self.ctxt_c = self.ctxt_lv_lstm(low_lv_lstm_outputs, final_hidden, self.ctxt_h, self.ctxt_c, seq_len = lengths)\n",
    "            tag_output.append(tag_size)\n",
    "            self.ctxt_vector = torch.squeeze(self.ctxt_vector, dim = 1)\n",
    "        return tag_output, self.ctxt_vector, self.ctxt_h, self.ctxt_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gjJnoCzw7i7"
   },
   "source": [
    "# Function: Train and evaluate LSTM models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 2113,
     "status": "ok",
     "timestamp": 1616791087389,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "p62Yl6TBw9Wr"
   },
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, early_stopping, dataloader, cuda, jagged, epochs=1):\n",
    "    \n",
    "    train_losses = []\n",
    "\n",
    "    flag = False\n",
    "    for i in range(epochs):\n",
    "        for j, data in enumerate(dataloader):\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            X = data['x']\n",
    "            Y = data['t']\n",
    "            L = data.get('length')\n",
    "            M = data.get('mask')\n",
    "            if cuda:\n",
    "                X = X.cuda()\n",
    "                Y = Y.cuda()\n",
    "                if jagged:\n",
    "                    L = L.cuda()\n",
    "                    M = M.cuda()\n",
    "            output, lstm_out, hx = model(X, lengths = L) \n",
    "            token_losses = criterion(output.transpose(1,2), Y)\n",
    "            if jagged:\n",
    "                masked_losses = token_losses * M\n",
    "                line_losses = torch.sum(masked_losses, dim = 1)\n",
    "            else:\n",
    "                line_losses = torch.mean(token_losses, dim = 1)\n",
    "                \n",
    "            loss = torch.mean(line_losses, dim = 0)\n",
    "\n",
    "\n",
    "            train_losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            early_stopping(loss, model)\n",
    "\n",
    "            if early_stopping.early_stop:\n",
    "                print(\"Early stopping\")\n",
    "                early_stopping.early_stop = False\n",
    "                early_stopping.counter = 0\n",
    "                flag = True\n",
    "                break\n",
    "            scheduler.step()\n",
    "        if flag == True:\n",
    "            break\n",
    "        \n",
    "        if i % 500 == 0:\n",
    "            print(f'{i}th epoch loss: {loss.item()}')\n",
    "    \n",
    "    model.load_state_dict(torch.load('checkpoint.pt'))\n",
    "\n",
    "    return model, train_losses\n",
    "\n",
    "def eval_model(model, criterion, dataloader, cuda, epochs=1):\n",
    "    \n",
    "    valid_losses = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        model.eval() # prep model for evaluation\n",
    "        for j, data in enumerate(dataloader):\n",
    "            X = data['x']\n",
    "            Y = data['t']\n",
    "            L = data.get('length')\n",
    "            M = data.get('mask')\n",
    "            if cuda:\n",
    "                X = X.cuda()\n",
    "                Y = Y.cuda()\n",
    "                if jagged:\n",
    "                    L = L.cuda()\n",
    "                    M = M.cuda()\n",
    "            output, lstm_out, hx = model(X, lengths = L)\n",
    "            token_losses = criterion(output.transpose(1,2), Y)\n",
    "            if jagged:\n",
    "                masked_losses = token_losses * M\n",
    "                line_losses = torch.sum(masked_losses, dim = 1)\n",
    "            else:\n",
    "                line_losses = torch.mean(token_losses, dim = 1)\n",
    "            loss = torch.mean(line_losses, dim = 0)\n",
    "            valid_losses.append(loss.item())\n",
    "    avg_valid_losses = np.mean(valid_losses)\n",
    "    print(f'Average validated loss: {avg_valid_losses}')\n",
    "        \n",
    "    return valid_losses, np.mean(valid_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tiered(model, criterion, optimizer, scheduler, early_stopping, data_handler, cuda, jagged, epochs=1):\n",
    "\n",
    "    for batch in data_handler:\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        if data_handler.empty == False:\n",
    "            X = batch['x']\n",
    "            C_V =  batch['context_vector']\n",
    "            C_H = batch['c_state_init']\n",
    "            C_C = batch['h_state_init']\n",
    "            Y = batch['t']\n",
    "            L = batch.get('length')\n",
    "            M = batch.get('mask')\n",
    "            if cuda:\n",
    "                X = X.cuda()\n",
    "                Y = Y.cuda()\n",
    "                if jagged:\n",
    "                    L = L.cuda()\n",
    "                    M = M.cuda()\n",
    "            tag_outputs, ctxt_vector, ctxt_h, ctxt_c = model(X, C_V, C_H, C_C, lengths = L)    \n",
    "            data_handler.update_state(ctxt_vector, ctxt_h, ctxt_c)\n",
    "\n",
    "            total_loss = 0\n",
    "            for output, true_y in zip(tag_outputs, Y):\n",
    "                token_losses = criterion(output.transpose(1,2), true_y)\n",
    "                if jagged:\n",
    "                    masked_losses = token_losses * M\n",
    "                    line_losses = torch.sum(masked_losses, dim = 1)\n",
    "                else:\n",
    "                    line_losses = torch.mean(token_losses, dim = 1)\n",
    "                loss = torch.mean(line_losses, dim = 0)\n",
    "                total_loss += loss\n",
    "                \n",
    "            total_loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            early_stopping(total_loss, model)\n",
    "        else:\n",
    "            print('Done')\n",
    "\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping\")\n",
    "            early_stopping.early_stop = False\n",
    "            early_stopping.counter = 0\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AZZoblRnxAmS"
   },
   "source": [
    "\n",
    "# Train model\n",
    "\n",
    "## Forward LSTM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11235,
     "status": "ok",
     "timestamp": 1616791096516,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "TuqWq1c6xBTb",
    "outputId": "23c787ba-c1df-44be-bfd8-0ccf5f9b2d23",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 0head.txt\n",
      "Validation loss decreased (inf --> 10.224283).  Saving model ...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-a3d0c40da38d>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, early_stopping, dataloader, cuda, jagged, epochs)\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlstm_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m             \u001b[0mtoken_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mjagged\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                 \u001b[0mmasked_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_losses\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1046\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1047\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0;32m-> 1048\u001b[0;31m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2688\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2689\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2690\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2691\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlog_softmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1670\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"log_softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1672\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1673\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Fwd_LSTM(layer_list, vocab_size, embedding_dim) #For bidirectional LSTM: bid_LSTM(layer_list, vocab_size, embedding_dim)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "criterion = nn.CrossEntropyLoss(reduction= 'none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 20, gamma=0.99)\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "full_losses= []\n",
    "epochs = 1\n",
    "for i, file in enumerate(files[:-1]):\n",
    "    print(f'Training on {file}')\n",
    "    dataset = LazyTextDataset(datafolder + file, sentence_length, skipsos, jagged, bid, delimiter)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, num_workers=0)\n",
    "    model, train_losses = train_model(model, criterion, optimizer, scheduler, early_stopping, dataloader, cuda, jagged, epochs)\n",
    "    full_losses = full_losses + train_losses\n",
    "    \n",
    "    print(f'Evaluating on {files[i+1]}')\n",
    "    dataset = LazyTextDataset(datafolder + files[i+1], sentence_length, skipsos, jagged, bid, delimiter)\n",
    "    dataloader = DataLoader(dataset, batch_size=128, num_workers=0)\n",
    "    valid_losses, avg_valid_loss = eval_model(model, criterion, dataloader, cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 11229,
     "status": "ok",
     "timestamp": 1616791096517,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "wgNW3kYvxD85",
    "outputId": "6c6abf92-1313-4a07-8d68-1d58bde4bd6b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y - Average loss')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(full_losses)\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('x - Number of batch')\n",
    "# naming the y axis\n",
    "plt.ylabel('y - Average loss')\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the tiered model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "context_size = [10]\n",
    "layer_list = [5] #hidden units of each layer.\n",
    "num_steps = 3\n",
    "model = Tiered_LSTM(layer_list, context_size, vocab_size, embedding_dim, context_size[0], jagged = False, bid = False)\n",
    "data_handler = OnlineLMBatcher(datafolder + files[0], conf, context_size, skipsos, jagged, bid, batch_size=64, num_steps=num_steps, delimiter=\" \", skiprows=0)\n",
    "criterion = nn.CrossEntropyLoss(reduction= 'none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 20, gamma=0.99)\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss decreased (inf --> 30.917603).  Saving model ...\n",
      "Validation loss decreased (30.917603 --> 30.903770).  Saving model ...\n",
      "Validation loss decreased (30.903770 --> 30.880672).  Saving model ...\n",
      "Validation loss decreased (30.880672 --> 30.860973).  Saving model ...\n",
      "Validation loss decreased (30.860973 --> 30.860498).  Saving model ...\n",
      "Validation loss decreased (30.860498 --> 30.848072).  Saving model ...\n",
      "Validation loss decreased (30.848072 --> 30.831627).  Saving model ...\n",
      "Validation loss decreased (30.831627 --> 30.817383).  Saving model ...\n",
      "Validation loss decreased (30.817383 --> 30.807428).  Saving model ...\n",
      "Validation loss decreased (30.807428 --> 30.784800).  Saving model ...\n",
      "Validation loss decreased (30.784800 --> 30.784048).  Saving model ...\n",
      "Validation loss decreased (30.784048 --> 30.754105).  Saving model ...\n",
      "Validation loss decreased (30.754105 --> 30.726372).  Saving model ...\n",
      "Validation loss decreased (30.726372 --> 30.650146).  Saving model ...\n",
      "Validation loss decreased (30.650146 --> 30.630285).  Saving model ...\n",
      "Validation loss decreased (30.630285 --> 30.609501).  Saving model ...\n",
      "Validation loss decreased (30.609501 --> 30.589931).  Saving model ...\n",
      "Validation loss decreased (30.589931 --> 30.577816).  Saving model ...\n",
      "Validation loss decreased (30.577816 --> 30.528011).  Saving model ...\n",
      "Validation loss decreased (30.528011 --> 30.496567).  Saving model ...\n",
      "Validation loss decreased (30.496567 --> 30.475183).  Saving model ...\n",
      "Validation loss decreased (30.475183 --> 30.469528).  Saving model ...\n",
      "Validation loss decreased (30.469528 --> 30.439110).  Saving model ...\n",
      "Validation loss decreased (30.439110 --> 30.416920).  Saving model ...\n",
      "Validation loss decreased (30.416920 --> 30.390652).  Saving model ...\n",
      "Validation loss decreased (30.390652 --> 30.385960).  Saving model ...\n",
      "Validation loss decreased (30.385960 --> 30.347380).  Saving model ...\n",
      "Validation loss decreased (30.347380 --> 30.250216).  Saving model ...\n",
      "Validation loss decreased (30.250216 --> 30.133232).  Saving model ...\n",
      "Validation loss decreased (30.133232 --> 30.097462).  Saving model ...\n",
      "Validation loss decreased (30.097462 --> 30.067242).  Saving model ...\n",
      "Validation loss decreased (30.067242 --> 30.050198).  Saving model ...\n",
      "Validation loss decreased (30.050198 --> 30.020958).  Saving model ...\n",
      "Validation loss decreased (30.020958 --> 29.975557).  Saving model ...\n",
      "Validation loss decreased (29.975557 --> 29.971405).  Saving model ...\n",
      "Validation loss decreased (29.971405 --> 29.956730).  Saving model ...\n",
      "Validation loss decreased (29.956730 --> 29.926994).  Saving model ...\n",
      "Validation loss decreased (29.926994 --> 29.900230).  Saving model ...\n",
      "Validation loss decreased (29.900230 --> 29.787376).  Saving model ...\n",
      "Validation loss decreased (29.787376 --> 29.707197).  Saving model ...\n",
      "Validation loss decreased (29.707197 --> 29.674345).  Saving model ...\n",
      "Validation loss decreased (29.674345 --> 29.666241).  Saving model ...\n",
      "Validation loss decreased (29.666241 --> 29.472588).  Saving model ...\n",
      "Validation loss decreased (29.472588 --> 29.316364).  Saving model ...\n",
      "Validation loss decreased (29.316364 --> 29.261326).  Saving model ...\n",
      "Validation loss decreased (29.261326 --> 29.105915).  Saving model ...\n",
      "Validation loss decreased (29.105915 --> 29.068880).  Saving model ...\n",
      "Validation loss decreased (29.068880 --> 28.861488).  Saving model ...\n",
      "Validation loss decreased (28.861488 --> 28.666042).  Saving model ...\n",
      "Validation loss decreased (28.666042 --> 28.499466).  Saving model ...\n",
      "Validation loss decreased (28.499466 --> 28.387484).  Saving model ...\n",
      "Validation loss decreased (28.387484 --> 28.279564).  Saving model ...\n",
      "Validation loss decreased (28.279564 --> 28.208706).  Saving model ...\n",
      "Validation loss decreased (28.208706 --> 28.112869).  Saving model ...\n",
      "Validation loss decreased (28.112869 --> 28.050858).  Saving model ...\n",
      "Validation loss decreased (28.050858 --> 27.968037).  Saving model ...\n",
      "Validation loss decreased (27.968037 --> 27.882296).  Saving model ...\n",
      "Validation loss decreased (27.882296 --> 27.828804).  Saving model ...\n",
      "Validation loss decreased (27.828804 --> 27.770954).  Saving model ...\n",
      "Validation loss decreased (27.770954 --> 27.661890).  Saving model ...\n",
      "Validation loss decreased (27.661890 --> 27.622793).  Saving model ...\n",
      "Validation loss decreased (27.622793 --> 27.518566).  Saving model ...\n",
      "Validation loss decreased (27.518566 --> 27.315355).  Saving model ...\n",
      "Validation loss decreased (27.315355 --> 27.266117).  Saving model ...\n",
      "Validation loss decreased (27.266117 --> 26.831871).  Saving model ...\n",
      "Validation loss decreased (26.831871 --> 26.686331).  Saving model ...\n",
      "Validation loss decreased (26.686331 --> 26.585838).  Saving model ...\n",
      "Validation loss decreased (26.585838 --> 26.429623).  Saving model ...\n",
      "Validation loss decreased (26.429623 --> 26.326708).  Saving model ...\n",
      "Validation loss decreased (26.326708 --> 26.261862).  Saving model ...\n",
      "Validation loss decreased (26.261862 --> 26.196093).  Saving model ...\n",
      "Validation loss decreased (26.196093 --> 26.066387).  Saving model ...\n",
      "Validation loss decreased (26.066387 --> 25.939943).  Saving model ...\n",
      "Validation loss decreased (25.939943 --> 25.908339).  Saving model ...\n",
      "Validation loss decreased (25.908339 --> 25.843193).  Saving model ...\n",
      "Validation loss decreased (25.843193 --> 25.842402).  Saving model ...\n",
      "Validation loss decreased (25.842402 --> 25.715057).  Saving model ...\n",
      "Validation loss decreased (25.715057 --> 25.650028).  Saving model ...\n",
      "Validation loss decreased (25.650028 --> 25.617298).  Saving model ...\n",
      "Validation loss decreased (25.617298 --> 25.583666).  Saving model ...\n",
      "Validation loss decreased (25.583666 --> 25.515799).  Saving model ...\n",
      "Validation loss decreased (25.515799 --> 25.447975).  Saving model ...\n",
      "Validation loss decreased (25.447975 --> 25.348244).  Saving model ...\n",
      "Validation loss decreased (25.348244 --> 25.184315).  Saving model ...\n",
      "Validation loss decreased (25.184315 --> 9.508906).  Saving model ...\n",
      "Validation loss decreased (9.508906 --> 9.397297).  Saving model ...\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "train_tiered(model, criterion, optimizer, scheduler, early_stopping, data_handler, cuda, jagged, epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OkOOqtkYxGWy"
   },
   "source": [
    "# Check models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11223,
     "status": "ok",
     "timestamp": 1616791096518,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "GmgoSqb5xINj",
    "outputId": "1d838f19-479e-4e05-8fb4-b883894c6c03"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: tensor([   4, 1181,    4,   18,   18,    6,   16,   21,    9,    1])\n",
      "Model prediction: tensor([[ 159,    6,  159,    6, 6030,  319,  187, 4979,   16,    1]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "dataset = LazyTextDataset(datafolder + file, sentence_length, skipsos, jagged, bid, delimiter)\n",
    "model.eval()\n",
    "test_input = torch.unsqueeze(dataset[1]['x'], dim=0)\n",
    "if cuda:\n",
    "    test_input = test_input.cuda()\n",
    "output, lstm_out, hx = model(test_input)\n",
    "print(f\"Ground truth: {dataset[1]['t']}\")\n",
    "print(f'Model prediction: {torch.argmax(output, dim=2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gdp0FidFQhu3"
   },
   "source": [
    "### (By overfitting LSTM model on a small dataset, let me check whether the model has ability to learn the relation between input and output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 11534,
     "status": "ok",
     "timestamp": 1616791096836,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "-1B5aPzMxJlT"
   },
   "outputs": [],
   "source": [
    "# mb_size = 1\n",
    "test_batch_size = 1\n",
    "sos = torch.zeros(mb_size * test_batch_size, 1, dtype=torch.long)\n",
    "eos = torch.ones(mb_size * test_batch_size, 1, dtype=torch.long)\n",
    "ex_sentences = torch.LongTensor(mb_size * test_batch_size, conf['sentence_length']-2 ).random_(0, vocab_size)\n",
    "example_sentences = torch.cat((sos, ex_sentences, eos), axis=1)\n",
    "\n",
    "startx = int(skipsos)+ int(jagged)\n",
    "startt = int(skipsos)+ int(jagged) + 1\n",
    "endx = example_sentences.shape[1]- int(not(bid))\n",
    "endt = example_sentences.shape[1] - int(bid)\n",
    "\n",
    "input_sentences = np.split(example_sentences[:,startx:endx], test_batch_size)\n",
    "output_sentences = np.split(example_sentences[:,startt:endt], test_batch_size)\n",
    "data = []\n",
    "\n",
    "for x, t in zip(input_sentences, output_sentences):\n",
    "    tmp_dict={}\n",
    "    tmp_dict['x'] = x\n",
    "    tmp_dict['t'] = t\n",
    "    data.append(tmp_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230187,
     "status": "ok",
     "timestamp": 1616791315493,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "cOM7J2Y_xK67",
    "outputId": "816014f3-6122-4e12-a120-97c2d928c7a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0th epoch loss: 10.261510848999023\n",
      "500th epoch loss: 5.311619758605957\n",
      "1000th epoch loss: 3.0353987216949463\n",
      "1500th epoch loss: 1.9604352712631226\n",
      "CPU times: user 34.7 s, sys: 21.9 s, total: 56.6 s\n",
      "Wall time: 3min 38s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Fwd_LSTM(layer_list, vocab_size, embedding_dim) #For bidirectional LSTM bid_LSTM(layer_list, vocab_size, embedding_dim)\n",
    "if cuda:\n",
    "    model.cuda()\n",
    "criterion = nn.CrossEntropyLoss(reduction = 'none')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size= 20, gamma=1)\n",
    "early_stopping = EarlyStopping(patience=patience, verbose=False)\n",
    "epochs = 3000\n",
    "model, train_losses = train_model(model, criterion, optimizer, scheduler, early_stopping, data, cuda, jagged, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 230183,
     "status": "ok",
     "timestamp": 1616791315494,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "lyMnmxlJKxBP",
    "outputId": "2597d9cc-4d54-4003-99c1-5e451a619137"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'y - Average loss')"
      ]
     },
     "execution_count": 16,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_losses)\n",
    "\n",
    "# naming the x axis\n",
    "plt.xlabel('x - Number of batch')\n",
    "# naming the y axis\n",
    "plt.ylabel('y - Average loss')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 230515,
     "status": "ok",
     "timestamp": 1616791315830,
     "user": {
      "displayName": "김영우",
      "photoUrl": "",
      "userId": "05084645099903710941"
     },
     "user_tz": -60
    },
    "id": "cgNPbcpU57Zp",
    "outputId": "696824d1-e674-421f-c2cd-aa43c965d854"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground truth: \n",
      " tensor([[18267, 26759, 12659, 14223,  1637, 10081, 25518,  6155, 10320,     1],\n",
      "        [25697, 18728, 27357, 11398,   170,    10, 24543, 26286,  9296,     1],\n",
      "        [16580, 18845, 13587,  3908, 15255,  1532, 17531,  9731, 28102,     1],\n",
      "        [13651, 15831, 12225, 15163, 18350, 13931, 18323, 23112, 13144,     1],\n",
      "        [ 7045, 21756, 13765, 21227,  8815, 25386, 26296, 26677,  5014,     1]])\n",
      "Model prediction: \n",
      " tensor([[24771, 26759, 12659, 14223,  1637, 10081, 25518,  6155, 10320,     1],\n",
      "        [25697, 18728, 27357, 11398,   170,    10, 24543, 26286,  9296,     1],\n",
      "        [16580, 18845, 13587,  3908, 15255,  1532, 17531,  9731, 28102,     1],\n",
      "        [13651, 15831, 12225, 15163, 18350, 22861, 18323, 23112, 13144,     1],\n",
      "        [ 7045, 21756, 13765, 21227,  8815, 25386, 26296, 26677,  5014,     1]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "test_input = data[0]['x'].cuda()\n",
    "if cuda:\n",
    "    test_input = test_input.cuda()\n",
    "output, lstm_out, hx = model(test_input[:5,:])\n",
    "print(f\"Ground truth: \\n {data[0]['t'][:5,:]}\")\n",
    "print(f'Model prediction: \\n {torch.argmax(output, dim=2)}')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyN1rIDpHKr5dyYDqgfJHqEe",
   "collapsed_sections": [],
   "mount_file_id": "1XofCw7SikjyLrq8NHxPt_E_C2eQksVny",
   "name": "Test_code(Dataloader + LSTM + Training loop).ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
