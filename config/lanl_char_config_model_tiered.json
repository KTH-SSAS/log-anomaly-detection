{
	"layers": [
		10
	],
	"context_layers": [
		10
	],
	"attention_type": null,
	"attention_dim": 0,
	"embedding_dim": 20,
	"vocab_size": 96,
	"sequence_length": null
}